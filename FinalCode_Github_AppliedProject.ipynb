{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7426463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--1\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "df = pd.read_csv(r\"AppInvData-Clean.csv\", sep=\"`\")\n",
    "fp = open('unique_app_names.csv', 'w', encoding=\"utf-8\")\n",
    "x = df ['ApplicationName'].unique().tolist()\n",
    "\n",
    "#df = pd.read_csv('uniq_app_names.txt', header=None)\n",
    "#x = df[0].tolist()\n",
    "\n",
    "ascii_special_chars = re.escape(string.punctuation)  # Escapes ASCII special characters\n",
    "# Adding Unicode punctuation, explicitly including full-width parentheses and colon\n",
    "unicode_punctuation = r'\\u3000-\\u303f\\u2000-\\u206f\\uff1a\\uff08\\uff09'  # Unicode punctuation + full-width colon and parentheses\n",
    "\n",
    "# Combine ASCII and Unicode punctuation\n",
    "special_chars = ascii_special_chars + unicode_punctuation\n",
    "\n",
    "# Escapes digits\n",
    "digits = re.escape(string.digits)\n",
    "\n",
    "# Regular expression patterns\n",
    "chinese_korean_japanese_chars = r'[\\u4e00-\\u9fff\\uac00-\\ud7af\\u3040-\\u30ff]'  # Chinese, Korean, Japanese (Hiragana, Katakana, Kanji)\n",
    "english_chars = r'[a-zA-Z]'  # English characters\n",
    "\n",
    "\n",
    "# Special words like version, 32-bit, 64-bit, x64, x32, beta\n",
    "#check_patterns = r'version|32-bit|64-bit|x64|x32|beta|versione|version2|fullversion|versions|trainingversion|oldwindowsversions|\\(free_version\\)|version[\\s._09]*|version_|versionsua|versionsua[\\W]*|netzversion|versioned|specialversion|bbeta|bversion|bsubversion'\n",
    "#check_patterns = r'version(?:e|s|2|_.*)?'\n",
    "check_patterns = (\n",
    "    r'\\b(?:version|32-bit|64-bit|x64|x32|bit|bits|beta|versione|version2|fullversion|'\n",
    "    r'versions|trainingversion|oldwindowsversions|\\(free_version\\)|'\n",
    "    r'version[\\s._09]*|version_|versionsua|versionsua[\\W]*|netzversion|'\n",
    "    r'versioned|specialversion|bbeta|bversion|bsubversion|subversion|'\n",
    "    r'versionneuse|fwversion|versiones|subversion x|betaversion|'\n",
    "    r'subversion r|simversion v|non versioned|specialversion)\\b'\n",
    ")\n",
    "\n",
    "# Function to remove quotes at the beginning and end of the text\n",
    "def remove_quotes(text):\n",
    "    text = text.strip()                                      # First, remove any leading/trailing spaces\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for i in x:\n",
    "\n",
    "#1. Task -convert app names to all lower case\n",
    "    i = str(i).lower()\n",
    "    #print(i)                                                 # Print the lowercase result\n",
    "\n",
    "#3.Task - Remove all special characters and digits regardless of the language\n",
    "    i1 = re.sub(f'[{special_chars}]', ' ', i)                 # Replace special characters with space\n",
    "    i2 = re.sub(f'[{digits}]', ' ', i1)                       # Replace digits with space\n",
    "    #print(i2)                                                # Print result after removing special characters & digits by replacing with space\n",
    "\n",
    "#4.Task - Check if the string contains only English characters or both english and chinese/korean then remove chinese/korean\n",
    "    if re.search(english_chars, i2):\n",
    "        my_new_string = re.sub(chinese_korean_japanese_chars, '', i2)  # If there are English characters, remove only Chinese and Korean characters\n",
    "    else:\n",
    "        my_new_string = i2                                    # If there are no English characters, keep Chinese/Korean but with special characters and digits removed\n",
    "\n",
    "#5. Task - replace multiple white spaces like \"    \" with a single white space \" \"\n",
    "    my_new_string = re.sub(r'\\s{2,}', ' ', my_new_string)\n",
    "\n",
    "#6. Task - Remove quotes at the beginning and end of the string\n",
    "    my_new_string = remove_quotes(my_new_string)\n",
    "    my_new_string = my_new_string.strip()                    # remove white spaces from the beginning and end of the app name\n",
    "    clean_data.append(my_new_string)                         # Add the cleaned app name to the list\n",
    "\n",
    "\n",
    "# Convert the cleaned list back to a DataFrame\n",
    "clean_df = pd.DataFrame(clean_data, columns=[\"Cleaned_App_Names\"])\n",
    "\n",
    "#2. Task - Remove the specified words/patterns using regex substitution\n",
    "# The regex will now exclude any words that start with the same prefix as those in check_patterns but will keep conversion\n",
    "clean_df[\"Cleaned_App_Names\"] = clean_df[\"Cleaned_App_Names\"].replace(\n",
    "    rf'(?<!\\w)({check_patterns})(?!\\w)', '', regex=True\n",
    ")\n",
    "\n",
    "clean_df[\"Cleaned_App_Names\"] = clean_df[\"Cleaned_App_Names\"].str.strip()\n",
    "\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "\n",
    "# Remove duplicates from the cleaned DataFrame\n",
    "distinct_df = clean_df.drop_duplicates(subset=[\"Cleaned_App_Names\"]).reset_index(drop=True)\n",
    "\n",
    "# Save the cleaned DataFrame to a new file\n",
    "#clean_df.to_csv(r\"C:\\Users\\yellu\\Downloads\\IFT593-Applied Project HW\\uniqA_app_names.txt\", header=False, index=False)\n",
    "distinct_df.to_csv(\"dc_step1.csv\", index=False)\n",
    "#distinct_df.to_csv(\"uniqAF_app_names.txt\", index=False)\n",
    "\n",
    "print(\"Data cleaned and saved to dc_step1.csv\")\n",
    "\n",
    "x = [str(i) for i in x]\n",
    "xsorted = sorted(x)\n",
    "for i in xsorted:\n",
    "    fp.write(str(i)+\"\\n\")\n",
    "fp.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d292ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--2\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Read the CSV file containing old app names\n",
    "df = pd.read_csv(r\"AppInvData-Clean.csv\", sep=\"`\", low_memory=False)\n",
    "\n",
    "# List of old app names from the CSV\n",
    "old_app_names = df['ApplicationName'].tolist()\n",
    "\n",
    "# Cleaned app names logic (reviewed for specific cleaning)\n",
    "ascii_special_chars = re.escape(string.punctuation)\n",
    "unicode_punctuation = r'\\u3000-\\u303f\\u2000-\\u206f\\uff1a\\uff08\\uff09'\n",
    "special_chars = ascii_special_chars + unicode_punctuation\n",
    "digits = re.escape(string.digits)\n",
    "\n",
    "# Only match unnecessary version-related words (tune this for better results)\n",
    "check_patterns = (\n",
    "    r'\\b(?:32-bit|64-bit|x64|x32|bit|bits|beta|versione|version2|fullversion|'\n",
    "    r'trainingversion|oldwindowsversions|\\(free_version\\)|version[\\s._09]*|'\n",
    "    r'versionsua|versionsua[\\W]*|netzversion|versionsed|specialversion)\\b'\n",
    ")\n",
    "\n",
    "# Function to remove unnecessary quotes at the beginning and end\n",
    "def remove_quotes(text):\n",
    "    text = text.strip()\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "# Iterate through old app names and clean\n",
    "for i in old_app_names:\n",
    "    i = str(i).lower()  # Convert to lowercase\n",
    "    i1 = re.sub(f'[{special_chars}]', ' ', i)  # Replace special characters with a space\n",
    "    i2 = re.sub(f'[{digits}]', ' ', i1)  # Replace digits with a space\n",
    "\n",
    "    # Make sure we remove only unnecessary patterns but keep important terms\n",
    "    my_new_string = re.sub(check_patterns, '', i2)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    my_new_string = re.sub(r'\\s{2,}', ' ', my_new_string)\n",
    "    \n",
    "    # Remove quotes if present\n",
    "    my_new_string = remove_quotes(my_new_string).strip()\n",
    "    \n",
    "    # Append to clean data list\n",
    "    clean_data.append(my_new_string)\n",
    "\n",
    "# Create a cleaned DataFrame\n",
    "clean_df = pd.DataFrame(clean_data, columns=[\"Cleaned_App_Names\"])\n",
    "\n",
    "# Make sure cleaned data still aligns properly with old data\n",
    "if len(old_app_names) != len(clean_df['Cleaned_App_Names']):\n",
    "    print(\"Mismatch between old and cleaned app names.\")\n",
    "else:\n",
    "    print(\"App name lists are aligned.\")\n",
    "\n",
    "# Align and create the dictionary\n",
    "old_to_new = {old_app_names[i]: clean_df['Cleaned_App_Names'][i] for i in range(len(old_app_names))}\n",
    "\n",
    "# Create a comparison DataFrame--task2\n",
    "df['New_ApplicationName'] = df['ApplicationName'].apply(lambda name: old_to_new.get(name, name))\n",
    "\n",
    "# Save the comparison DataFrame to a CSV with a different name to avoid potential file locking issues--task2\n",
    "df[['ApplicationName', 'New_ApplicationName']].to_csv('old_vs_new_app_names_1.csv', index=False)\n",
    "\n",
    "df['ApplicationName'] = df['New_ApplicationName']\n",
    "\n",
    "# Save the updated DataFrame to a CSV file--task2\n",
    "df.to_csv('replaced_old_with_new.csv', index=False)\n",
    "\n",
    "#Replacing old app names with new ones--task2\n",
    "print(\"Comparison of old and new app names saved to 'old_vs_new_app_names_1.csv'.\")\n",
    "print(\"Replaced Old with new in main csv file to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--3\n",
    "\n",
    "#Remove unnecesary columns like below ---task3 \n",
    "\n",
    "#df = df.drop(columns=['Unnamed: 0', 'ApplicationKey','ApplicationId','ApplicationVersion','ApplicationPublisher','New_ApplicationName'])\n",
    "df = df.drop(columns=['ApplicationKey','ApplicationId','ApplicationVersion','ApplicationPublisher','New_ApplicationName'])\n",
    "\n",
    "\n",
    "#Remove the rows with empty username------task3 \n",
    "cleaned_df_username =df.dropna(subset=['Clean User Name'])\n",
    "\n",
    "#save updated dataframe after removing uncessary column and removing rows with no clean username value-------task3 \n",
    "cleaned_df_username.to_csv('removed_unecessary_user_names_columns2.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Removing uncessary column and removing rows with no clean username value and saving to 'removed_unecessary_user_names_columns2.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--4\n",
    "\n",
    "# Get the frequency counts of each application\n",
    "application_counts = cleaned_df_username['ApplicationName'].value_counts()\n",
    "\n",
    "thresholds = [0.00]\n",
    "\n",
    "# Loop through each threshold to filter and save results\n",
    "for threshold in thresholds:\n",
    "    # Calculate the rank cutoff for the top X% most popular applications\n",
    "    cutoff_rank = int(len(application_counts) * threshold)\n",
    "    \n",
    "    # Get the list of top X% applications to remove\n",
    "    applications_to_remove = application_counts.nlargest(cutoff_rank).index\n",
    "    \n",
    "    # Filter out the top X% applications\n",
    "    filtered_df = cleaned_df_username[~cleaned_df_username['ApplicationName'].isin(applications_to_remove)]\n",
    "    \n",
    "    # Print information about the filtering for verification\n",
    "    print(f\"\\nThreshold: {int(threshold * 100)}%\")\n",
    "    print(f\"Top {int(threshold * 100)}% applications removed: {len(applications_to_remove)}\")\n",
    "    print(f\"Size of filtered dataset: {filtered_df.memory_usage(deep=True).sum() / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # Save the filtered data to a CSV file with the threshold in the filename\n",
    "    output_filename = f'filtered_applications_{int(threshold * 100)}cutoff.csv'\n",
    "    filtered_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved file: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696303c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the filtered applications data\n",
    "filtered_df = pd.read_csv(\"filtered_applications_0cutoff.csv\")\n",
    "\n",
    "# Step 2: Load the original data and select required columns\n",
    "original_df = pd.read_csv(\"AD-user-Clean.csv\", sep=\"`\")\n",
    "#selected_columns = original_df[['SBU', 'SBG', 'user.hwSBXDescription', 'SBE', 'organizationalPerson.department', 'CO-3Char', 'Clean User Name', 'user.hwJobTitle']]\n",
    "\n",
    "# Step 3: Merge the filtered data with selected columns from the original file on 'Clean User Name'\n",
    "merged_df = pd.merge(filtered_df, selected_columns, on=\"Clean User Name\", how=\"inner\")\n",
    "\n",
    "# Step 4: Save the merged DataFrame to a new file\n",
    "merged_df.to_csv(\"merged_filtered_applications_0cutoff.csv\", index=False)\n",
    "\n",
    "print(\"Merged filtered applications with selected columns and saved to 'merged_filtered_applications_0cutoff.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--6\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Loading the file \"merged_filtered_applications_0cutoff\" if available in the environment\n",
    "df = pd.read_csv(r\"C:\\Users\\ANN\\Downloads\\\\merged_filtered_applications_0cutoff.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--7\n",
    "\n",
    "#We are finding unique applications and it's freqeuncies based on clean user name\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'merged_filtered_applications_0cutoff.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Group by ApplicationName and count unique Clean User Name entries\n",
    "frequency = data.groupby('ApplicationName')['Clean User Name'].nunique().reset_index(name='UniqueUserCount')\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_file = 'application_name_unique_frequency_count.csv'\n",
    "frequency.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Unique user count per application saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--8\n",
    "\n",
    "#Based on the above output file we are removing apps which are Top 75%(10085 users) and apps installed by less than 25 users\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'application_name_unique_frequency_count.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate total number of users based on the sum of the 'UserCount' column\n",
    "total_users = df['UniqueUserCount'].sum()\n",
    "\n",
    "# Filter out applications used Top 75% frequent Users \n",
    "df = df[df['UniqueUserCount'] <= 10085]\n",
    "\n",
    "# Filter out applications used by 25 users or fewer\n",
    "df = df[df['UniqueUserCount'] > 25]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "output_path = 'Top_75%apps_less_25users.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a54e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--9\n",
    "\n",
    "#Merged both datasets - Less_25%apps_less_25users.csv and merged_filtered_applications_0cutoff.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load both datasets\n",
    "app_user_count_df = pd.read_csv(\"Top_75%apps_less_25users.csv\")\n",
    "merged_data_df = pd.read_csv(\"merged_filtered_applications_0cutoff.csv\")\n",
    "\n",
    "# Step 2: Merge both datasets on 'ApplicationName' column to associate unique user counts with each app's user data\n",
    "merged_df = pd.merge(merged_data_df, app_user_count_df, on='ApplicationName', how='inner')\n",
    "\n",
    "# Step 3: Filter rows based on the specified number of unique users for each application\n",
    "# Create a cumulative count of users within each 'ApplicationName' group and filter based on 'UniqueUserCount'\n",
    "merged_df['user_count_within_app'] = merged_df.groupby('ApplicationName').cumcount() + 1\n",
    "filtered_df = merged_df[merged_df['user_count_within_app'] <= merged_df['UniqueUserCount']]\n",
    "\n",
    "# Step 4: Select only the required columns for the output\n",
    "output = filtered_df[['ApplicationName', 'Clean User Name', 'Clean PC Name', 'SBU', 'SBG', \n",
    "                      'user.hwSBXDescription', 'SBE', 'organizationalPerson.department', \n",
    "                      'CO-3Char', 'user.hwJobTitle']]\n",
    "\n",
    "# Step 5: Save the final output to a CSV file\n",
    "output_path = \"filtered_Top_75%apps_less_25users.csv\"\n",
    "output.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--10\n",
    "\n",
    "# Encoded columns ares created for each decoded column\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the CSV file with decoded columns\n",
    "file_path = 'filtered_Top_75%apps_less_25users.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# List of columns to encode\n",
    "columns_to_encode = [\n",
    "    'ApplicationName', 'Clean User Name', 'Clean PC Name', 'SBU', 'SBG', \n",
    "    'user.hwSBXDescription', 'SBE', 'organizationalPerson.department', \n",
    "    'CO-3Char', 'user.hwJobTitle'\n",
    "]\n",
    "\n",
    "# Initialize a LabelEncoder for each column and create encoded versions\n",
    "for column in columns_to_encode:\n",
    "    # Initialize the label encoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit and transform the data in the column\n",
    "    df[column + '_encoded'] = le.fit_transform(df[column])\n",
    "    \n",
    "    # Insert the encoded column next to the original column\n",
    "    original_col_index = df.columns.get_loc(column)\n",
    "    encoded_col = df.pop(column + '_encoded')\n",
    "    df.insert(original_col_index + 1, column + '_encoded', encoded_col)\n",
    "    \n",
    "# Save the updated DataFrame with encoded columns to a new CSV file\n",
    "output_path = 'encoded_decoded_col.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Encoded data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--10\n",
    "\n",
    "#Removing the duplicates for the combination of clean pc name and application name\n",
    "import pandas as pd\n",
    "\n",
    "def remove_duplicates(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Removes duplicates based on the combination of Clean PC Name and ApplicationName.\n",
    "    Saves the resulting file to a new CSV.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove duplicates based on the combination of 'Clean PC Name' and 'ApplicationName'\n",
    "    df_cleaned = df.drop_duplicates(subset=['Clean PC Name', 'ApplicationName'])\n",
    "\n",
    "    # Save the cleaned DataFrame to a new file\n",
    "    df_cleaned.to_csv(output_file, index=False)\n",
    "    print(f\"Duplicates removed. Cleaned data saved to {output_file}\")\n",
    "\n",
    "# File paths\n",
    "input_file = 'encoded_decoded_col.csv'   # Input file path\n",
    "output_file = 'cleaned_encoded_decoded_col.csv'         # Output file path\n",
    "\n",
    "# Execute the function\n",
    "remove_duplicates(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5572544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--11\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "# Load the dataset with pre-encoded columns\n",
    "file_path = 'cleaned_encoded_decoded_col.csv'\n",
    "filtered_df = pd.read_csv(file_path)\n",
    "\n",
    "# Increment the encoded user indices to start from 1\n",
    "row_indices = filtered_df['Clean User Name_encoded'] + 1  # Start from 1\n",
    "col_indices = filtered_df['ApplicationName_encoded']\n",
    "data_values = [1] * len(filtered_df)  # Each interaction marked by 1\n",
    "\n",
    "# Define the shape of the sparse matrix based on the maximum values in the encoded columns\n",
    "num_users = row_indices.max() + 1  # Adjust for one-based indexing\n",
    "num_apps = col_indices.max() + 1\n",
    "\n",
    "# Create the sparse matrix with all interactions for each user-application pair\n",
    "user_app_matrix = csr_matrix((data_values, (row_indices, col_indices)), shape=(num_users, num_apps))\n",
    "\n",
    "# Generate target labels as the username_encoded for each user\n",
    "# Start from 1 as the user indices were incremented\n",
    "target_labels = pd.Series(range(1, num_users + 1))\n",
    "\n",
    "# Ensure the first row has a target label\n",
    "if target_labels.iloc[0] == 0:\n",
    "    target_labels.iloc[0] = 1\n",
    "\n",
    "# Convert to libSVM format and save with zero-based indexing\n",
    "output_file = 'Username_application_libsvm_format_filled.txt'\n",
    "dump_svmlight_file(user_app_matrix, target_labels, output_file, zero_based=True)\n",
    "\n",
    "print(f\"Data saved in libSVM format to '{output_file}' with the first row correctly filled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7218f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--11\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the LIBSVM file directly as a sparse matrix\n",
    "def load_libsvm_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads the LibSVM file into a sparse matrix format\n",
    "    \"\"\"\n",
    "    data, _ = load_svmlight_file(file_path)  # 'labels' are ignored for clustering\n",
    "    return data\n",
    "\n",
    "def plot_and_save_to_excel(kmeans, k, writer_top10, writer_all, mapping_df):\n",
    "    \"\"\"\n",
    "    Generates and saves the cluster data, top-10 apps, and histograms to Excel\n",
    "    \"\"\"\n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    # DataFrame for storing User ID to Cluster mapping\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'User ID Encoded': range(len(clusters)),\n",
    "        'Cluster': clusters\n",
    "    })\n",
    "\n",
    "    # Reset index for mapping_df to allow access to both index and columns\n",
    "    mapping_df_reset = mapping_df.reset_index()\n",
    "\n",
    "    # Process for each cluster\n",
    "    for cluster_num in range(k):\n",
    "        # Filter data for each cluster\n",
    "        cluster_data = user_app_matrix[user_cluster_df['Cluster'] == cluster_num]\n",
    "\n",
    "        # Top 10 applications by frequency for each cluster\n",
    "        top_app_counts = pd.Series(cluster_data.toarray().sum(axis=0)).nlargest(10)\n",
    "        all_app_counts = pd.Series(cluster_data.toarray().sum(axis=0)).sort_values(ascending=False)\n",
    "\n",
    "        # Map Application IDs to their names using mapping_df_reset\n",
    "        top_apps_with_names = mapping_df_reset.loc[mapping_df_reset['ApplicationName_encoded'].isin(top_app_counts.index)].copy()\n",
    "        top_apps_with_names['Frequency'] = top_app_counts.values\n",
    "        top_apps_with_names['Cluster'] = cluster_num\n",
    "\n",
    "        all_apps_with_names = mapping_df_reset.loc[mapping_df_reset['ApplicationName_encoded'].isin(all_app_counts.index)].copy()\n",
    "        all_apps_with_names['Frequency'] = all_app_counts.values\n",
    "        all_apps_with_names['Cluster'] = cluster_num\n",
    "\n",
    "        # Creating Excel entries for top 10 apps\n",
    "        top_apps_with_names.to_excel(writer_top10, sheet_name=f'K={k}', startrow=cluster_num * 15, index=False)\n",
    "\n",
    "        # Adding a histogram for the top 10 applications in each cluster\n",
    "        chart_top = writer_top10.book.add_chart({'type': 'column'})\n",
    "        chart_top.add_series({\n",
    "            'categories': [f'K={k}', cluster_num * 15 + 1, 1, cluster_num * 15 + 10, 1],  # Adjusted for top 10\n",
    "            'values': [f'K={k}', cluster_num * 15 + 1, 2, cluster_num * 15 + 10, 2],  # Adjusted for top 10\n",
    "            'name': f'Cluster {cluster_num + 1} - Top 10 Apps'\n",
    "        })\n",
    "        writer_top10.sheets[f'K={k}'].insert_chart(f'G{cluster_num * 15 + 1}', chart_top)\n",
    "\n",
    "        # Creating Excel entries for all applications\n",
    "        all_apps_with_names.to_excel(writer_all, sheet_name=f'K={k}', startrow=cluster_num * 15, index=False)\n",
    "\n",
    "        # Adding a histogram for all applications in each cluster\n",
    "        chart_all = writer_all.book.add_chart({'type': 'column'})\n",
    "        chart_all.add_series({\n",
    "            'categories': [f'K={k}', cluster_num * 15 + 1, 1, cluster_num * 15 + len(all_app_counts), 1],\n",
    "            'values': [f'K={k}', cluster_num * 15 + 1, 2, cluster_num * 15 + len(all_app_counts), 2],\n",
    "            'name': f'Cluster {cluster_num + 1} - All Apps'\n",
    "        })\n",
    "        writer_all.sheets[f'K={k}'].insert_chart(f'G{cluster_num * 15 + 1}', chart_all)\n",
    "\n",
    "# Step 3: Elbow Method for Optimal k by plotting inertia (sum of sq dist)\n",
    "def plot_elbow_method(k_values, inertia_values):\n",
    "    \"\"\"\n",
    "    Plots the Elbow Method to find the optimal number of clusters (k)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, inertia_values, marker='o')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(k_values)\n",
    "    plt.savefig('elbow_method_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Main clustering loop\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 4: Load the LIBSVM file\n",
    "    user_app_matrix = load_libsvm_file('Username_application_libsvm_format_filled.txt')\n",
    "\n",
    "    # Step 5: Load the mapping file to map Application IDs to Names\n",
    "    mapping_file = 'cleaned_encoded_decoded_col.csv'\n",
    "    mapping_df = pd.read_csv(mapping_file)\n",
    "    mapping_df = mapping_df[['ApplicationName_encoded', 'ApplicationName']].drop_duplicates().set_index('ApplicationName_encoded')\n",
    "\n",
    "    # Step 6: Initialize variables for the clustering process\n",
    "    k_values = range(5, 101, 5)  # K = 5, 10, 15, ..., 100\n",
    "    inertia_values = []  # List to store inertia values\n",
    "\n",
    "    # Step 7: Use XlsxWriter to create separate Excel files for top 10 and all applications\n",
    "    with pd.ExcelWriter('clustering_report_top10_apps_mapped.xlsx', engine='xlsxwriter') as writer_top10, \\\n",
    "         pd.ExcelWriter('clustering_report_all_apps_mapped.xlsx', engine='xlsxwriter') as writer_all:\n",
    "        for k in k_values:\n",
    "            print(f'Running clustering for K = {k}...')\n",
    "\n",
    "            # Apply MiniBatchKMeans clustering\n",
    "            kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, n_init=10, batch_size=100)\n",
    "            kmeans.fit(user_app_matrix)  # Fit the model on the sparse matrix\n",
    "\n",
    "            # Append the inertia (distortion) value for this k\n",
    "            inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "            # Add sheets for each K\n",
    "            writer_top10.book.add_worksheet(f'K={k}')\n",
    "            writer_all.book.add_worksheet(f'K={k}')\n",
    "\n",
    "            # Plot and save the results to the Excel files for top 10 and all applications\n",
    "            plot_and_save_to_excel(kmeans, k, writer_top10, writer_all, mapping_df)\n",
    "\n",
    "    # Step 8: Plot the elbow method graph\n",
    "    plot_elbow_method(k_values, inertia_values)\n",
    "    print(\"Excel reports with mapped application names generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task--12\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and output file names\n",
    "input_file = \"cleaned_encoded_decoded_col.csv\"\n",
    "output_file = \"top_10_applications_SBU.csv\"\n",
    "\n",
    "# Step 1: Load the input CSV file\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Preview the dataset to ensure it has required columns like 'SBU' and 'ApplicationName'\n",
    "print(data.head())\n",
    "\n",
    "# Step 2: Group by SBU and calculate application frequencies\n",
    "grouped = data.groupby(['SBU', 'ApplicationName']).size().reset_index(name='Frequency')\n",
    "\n",
    "# Step 3: Identify the top 10 applications in each SBU\n",
    "top_10_apps = []\n",
    "\n",
    "for sbu, group in grouped.groupby('SBU'):\n",
    "    # Sort applications by frequency\n",
    "    sorted_group = group.sort_values(by='Frequency', ascending=False)\n",
    "    # Select top 10 applications\n",
    "    top_apps = sorted_group.head(10)\n",
    "    top_10_apps.append(top_apps)\n",
    "\n",
    "# Combine all top applications into a single DataFrame\n",
    "top_10_apps_df = pd.concat(top_10_apps)\n",
    "\n",
    "# Step 4: Save the results to a new CSV file\n",
    "top_10_apps_df.to_csv(output_file, index=False)\n",
    "print(f\"Top 10 applications saved to {output_file}\")\n",
    "\n",
    "# Step 5: Visualize the data\n",
    "for sbu, group in top_10_apps_df.groupby('SBU'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(group['ApplicationName'], group['Frequency'], color='skyblue')\n",
    "    plt.title(f\"Top 10 Applications in {sbu}\")\n",
    "    plt.xlabel(\"ApplicationName\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
